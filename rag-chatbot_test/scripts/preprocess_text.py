"""
í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ìŠ¤í¬ë¦½íŠ¸

ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³  200-500 ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
"""

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict
import math


def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ì œ: Gutenberg í—¤ë”/í‘¸í„° ì œê±°, ì¸ì½”ë”© ì •ë¦¬
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
    
    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    # Gutenberg í”„ë¡œì íŠ¸ í—¤ë”/í‘¸í„° íŒ¨í„´ ì œê±°
    # í—¤ë” ì œê±° (ë³´í†µ "*** START OF ..." íŒ¨í„´)
    text = re.sub(r"\*\*\* START OF.*?\*\*\*", "", text, flags=re.DOTALL | re.IGNORECASE)
    
    # í‘¸í„° ì œê±° (ë³´í†µ "*** END OF ..." íŒ¨í„´)
    text = re.sub(r"\*\*\* END OF.*?\*\*\*", "", text, flags=re.DOTALL | re.IGNORECASE)
    
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r"\s+", " ", text)
    
    # ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\n\s*\n", "\n\n", text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text


def split_into_chunks(
    text: str,
    chunk_size: int = 400,
    chunk_overlap: int = 50,
    min_chunk_size: int = 200
) -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        text: ì •ì œëœ í…ìŠ¤íŠ¸
        chunk_size: ëª©í‘œ ì²­í¬ í¬ê¸° (ë‹¨ì–´ ìˆ˜)
        chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ìˆ˜
        min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ëŠ” text, word_count, chunk_index í¬í•¨)
    """
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°„ë‹¨í•œ ë°©ë²•)
    sentences = re.split(r'[.!?]+\s+', text)
    
    chunks = []
    current_chunk = []
    current_word_count = 0
    chunk_index = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # ë¬¸ì¥ì˜ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        words = sentence.split()
        sentence_word_count = len(words)
        
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í•˜ë©´ ëª©í‘œ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
        if current_word_count + sentence_word_count > chunk_size and current_chunk:
            # í˜„ì¬ ì²­í¬ ì €ì¥
            chunk_text = " ".join(current_chunk)
            if len(chunk_text.split()) >= min_chunk_size:
                chunks.append({
                    "chunk_index": chunk_index,
                    "text": chunk_text,
                    "word_count": current_word_count,
                    "char_count": len(chunk_text),
                })
                chunk_index += 1
            
            # ì˜¤ë²„ë© ì²˜ë¦¬: ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ì„ ë‹¤ìŒ ì²­í¬ ì‹œì‘ì ìœ¼ë¡œ
            if chunk_overlap > 0:
                overlap_words = []
                overlap_count = 0
                for sent in reversed(current_chunk):
                    sent_words = sent.split()
                    if overlap_count + len(sent_words) <= chunk_overlap:
                        overlap_words.insert(0, sent)
                        overlap_count += len(sent_words)
                    else:
                        break
                current_chunk = overlap_words
                current_word_count = overlap_count
            else:
                current_chunk = []
                current_word_count = 0
        
        # ë¬¸ì¥ ì¶”ê°€
        current_chunk.append(sentence)
        current_word_count += sentence_word_count
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
    if current_chunk:
        chunk_text = " ".join(current_chunk)
        if len(chunk_text.split()) >= min_chunk_size:
            chunks.append({
                "chunk_index": chunk_index,
                "text": chunk_text,
                "word_count": current_word_count,
                "char_count": len(chunk_text),
            })
    
    return chunks


def preprocess_book(input_file: str, output_dir: str, chunk_size: int = 400) -> Dict:
    """
    ë‹¨ì¼ ì±… íŒŒì¼ ì „ì²˜ë¦¬
    
    Args:
        input_file: ì…ë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        chunk_size: ì²­í¬ í¬ê¸° (ë‹¨ì–´ ìˆ˜)
    
    Returns:
        ì „ì²˜ë¦¬ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
    """
    input_path = Path(input_file)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ ì½ê¸°
    print(f"ğŸ“– íŒŒì¼ ì½ê¸°: {input_file}")
    with open(input_path, "r", encoding="utf-8") as f:
        raw_text = f.read()
    
    # ë©”íƒ€ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
    metadata_file = input_path.parent / f"{input_path.stem}.metadata.json"
    metadata = {}
    if metadata_file.exists():
        with open(metadata_file, "r", encoding="utf-8") as f:
            metadata = json.load(f)
    
    # í…ìŠ¤íŠ¸ ì •ì œ
    print("ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
    cleaned_text = clean_text(raw_text)
    
    # ì²­í‚¹
    print(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘ (ëª©í‘œ í¬ê¸°: {chunk_size} ë‹¨ì–´)...")
    chunks = split_into_chunks(cleaned_text, chunk_size=chunk_size)
    
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    
    # ì²­í¬ ì €ì¥
    book_id = metadata.get("gutenberg_id", input_path.stem)
    output_file = output_path / f"{book_id}_chunks.json"
    
    chunks_data = {
        "book_id": book_id,
        "title": metadata.get("title", input_path.stem),
        "author": metadata.get("author", "Unknown"),
        "total_chunks": len(chunks),
        "total_words": sum(c["word_count"] for c in chunks),
        "chunks": chunks,
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(chunks_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ì²­í¬ ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   ì´ ë‹¨ì–´ ìˆ˜: {chunks_data['total_words']:,}")
    print(f"   í‰ê·  ì²­í¬ í¬ê¸°: {chunks_data['total_words'] // len(chunks):,} ë‹¨ì–´")
    
    return chunks_data


def main():
    parser = argparse.ArgumentParser(description="í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--input",
        required=True,
        help="ì…ë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--output",
        default="data/processed",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/processed)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=400,
        help="ì²­í¬ í¬ê¸° (ë‹¨ì–´ ìˆ˜, ê¸°ë³¸: 400)"
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    
    if input_path.is_file():
        # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
        preprocess_book(str(input_path), args.output, args.chunk_size)
    elif input_path.is_dir():
        # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  .txt íŒŒì¼ ì²˜ë¦¬
        txt_files = list(input_path.glob("*.txt"))
        print(f"ğŸ“š {len(txt_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        for txt_file in txt_files:
            print(f"\n{'='*60}")
            preprocess_book(str(txt_file), args.output, args.chunk_size)
    else:
        print(f"âŒ ì…ë ¥ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")


if __name__ == "__main__":
    main()

