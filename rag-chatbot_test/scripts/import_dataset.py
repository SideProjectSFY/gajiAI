#!/usr/bin/env python
"""
VectorDB Dataset Import Script

Pre-processed Project Gutenberg ë°ì´í„°ì…‹ì„ ChromaDB/Pineconeì— ì„í¬íŠ¸í•©ë‹ˆë‹¤.
5ê°œì˜ ì»¬ë ‰ì…˜(novel_passages, characters, locations, events, themes)ì„ ìƒì„±í•˜ê³ 
PostgreSQL ë©”íƒ€ë°ì´í„°ë¥¼ Spring Boot APIë¥¼ í†µí•´ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/import_dataset.py \
        --dataset-path /path/to/gutenberg_dataset \
        --vectordb chromadb \
        --vectordb-host localhost:8001 \
        --spring-boot-api http://localhost:8080
"""

import argparse
import json
import os
import sys
import time
import uuid
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

try:
    import pandas as pd
except ImportError:
    print("âŒ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install pandas")
    sys.exit(1)

try:
    import chromadb
except ImportError:
    print("âŒ chromadb ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install chromadb")
    sys.exit(1)

try:
    import httpx
except ImportError:
    print("âŒ httpx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install httpx")
    sys.exit(1)

try:
    from tqdm import tqdm
except ImportError:
    # tqdmì´ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ëŒ€ì²´ êµ¬í˜„ ì‚¬ìš©
    def tqdm(iterable, desc="", total=None):
        """Simple tqdm fallback"""
        for i, item in enumerate(iterable):
            if total:
                print(f"\r{desc}: {i+1}/{total}", end="", flush=True)
            yield item
        print()


@dataclass
class ImportStats:
    """ì„í¬íŠ¸ í†µê³„"""
    novels_count: int = 0
    passages_count: int = 0
    characters_count: int = 0
    locations_count: int = 0
    events_count: int = 0
    themes_count: int = 0
    errors: List[str] = None

    def __post_init__(self):
        if self.errors is None:
            self.errors = []


class DatasetValidator:
    """ë°ì´í„°ì…‹ êµ¬ì¡° ë° ë¬´ê²°ì„± ê²€ì¦"""

    REQUIRED_FILES = ["novels.json"]
    OPTIONAL_DIRS = ["passages", "characters", "locations", "events", "themes"]
    EMBEDDING_DIM = 768  # Gemini Embedding API ì°¨ì›

    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)

    def validate(self) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ ê²€ì¦ ìˆ˜í–‰

        Returns:
            ê²€ì¦ ê²°ê³¼ dict
        """
        result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "stats": {}
        }

        # 1. í•„ìˆ˜ íŒŒì¼ í™•ì¸
        for required_file in self.REQUIRED_FILES:
            file_path = self.dataset_path / required_file
            if not file_path.exists():
                result["valid"] = False
                result["errors"].append(f"í•„ìˆ˜ íŒŒì¼ ì—†ìŒ: {required_file}")

        # 2. ì„ íƒ ë””ë ‰í† ë¦¬ í™•ì¸
        for optional_dir in self.OPTIONAL_DIRS:
            dir_path = self.dataset_path / optional_dir
            if dir_path.exists():
                result["stats"][optional_dir] = self._count_files(dir_path)
            else:
                result["warnings"].append(f"ì„ íƒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {optional_dir}")

        # 3. novels.json ë‚´ìš© ê²€ì¦
        novels_path = self.dataset_path / "novels.json"
        if novels_path.exists():
            novels_result = self._validate_novels(novels_path)
            result["stats"]["novels"] = novels_result["count"]
            result["errors"].extend(novels_result["errors"])
            if novels_result["errors"]:
                result["valid"] = False

        # 4. ì„ë² ë”© ì°¨ì› ê²€ì¦ (ìƒ˜í”Œ)
        passages_dir = self.dataset_path / "passages"
        if passages_dir.exists():
            embedding_result = self._validate_embeddings(passages_dir)
            result["errors"].extend(embedding_result["errors"])
            if embedding_result["errors"]:
                result["valid"] = False

        return result

    def _count_files(self, dir_path: Path) -> int:
        """ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ìˆ˜ ì¹´ìš´íŠ¸"""
        return len(list(dir_path.glob("*.json"))) + len(list(dir_path.glob("*.parquet")))

    def _validate_novels(self, novels_path: Path) -> Dict:
        """novels.json ê²€ì¦"""
        result = {"count": 0, "errors": []}

        try:
            with open(novels_path, "r", encoding="utf-8") as f:
                novels = json.load(f)

            if not isinstance(novels, list):
                result["errors"].append("novels.jsonì€ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
                return result

            result["count"] = len(novels)

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ["id", "title", "author"]
            for i, novel in enumerate(novels):
                for field in required_fields:
                    if field not in novel:
                        result["errors"].append(
                            f"novels[{i}]: í•„ìˆ˜ í•„ë“œ '{field}' ì—†ìŒ"
                        )

            # ì¤‘ë³µ ID ê²€ì‚¬
            novel_ids = [n.get("id") for n in novels if n.get("id")]
            if len(novel_ids) != len(set(novel_ids)):
                result["errors"].append("novels.jsonì— ì¤‘ë³µ IDê°€ ìˆìŠµë‹ˆë‹¤")

        except json.JSONDecodeError as e:
            result["errors"].append(f"novels.json JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            result["errors"].append(f"novels.json ì½ê¸° ì˜¤ë¥˜: {e}")

        return result

    def _validate_embeddings(self, passages_dir: Path) -> Dict:
        """ì„ë² ë”© ì°¨ì› ê²€ì¦ (ì²« ë²ˆì§¸ íŒŒì¼ ìƒ˜í”Œ)"""
        result = {"errors": []}

        # JSON ë˜ëŠ” Parquet íŒŒì¼ ì°¾ê¸°
        json_files = list(passages_dir.glob("*.json"))
        parquet_files = list(passages_dir.glob("*.parquet"))

        if json_files:
            try:
                with open(json_files[0], "r", encoding="utf-8") as f:
                    data = json.load(f)

                if isinstance(data, list) and len(data) > 0:
                    sample = data[0]
                elif isinstance(data, dict) and "passages" in data:
                    sample = data["passages"][0] if data["passages"] else None
                else:
                    sample = None

                if sample and "embedding" in sample:
                    dim = len(sample["embedding"])
                    if dim != self.EMBEDDING_DIM:
                        result["errors"].append(
                            f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: {dim} (expected: {self.EMBEDDING_DIM})"
                        )
            except Exception as e:
                result["errors"].append(f"ì„ë² ë”© ê²€ì¦ ì˜¤ë¥˜: {e}")

        elif parquet_files:
            try:
                df = pd.read_parquet(parquet_files[0])
                if "embedding" in df.columns and len(df) > 0:
                    embedding = df.iloc[0]["embedding"]
                    if hasattr(embedding, "__len__"):
                        dim = len(embedding)
                        if dim != self.EMBEDDING_DIM:
                            result["errors"].append(
                                f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: {dim} (expected: {self.EMBEDDING_DIM})"
                            )
            except Exception as e:
                result["errors"].append(f"Parquet ì„ë² ë”© ê²€ì¦ ì˜¤ë¥˜: {e}")

        return result


class GutenbergDatasetImporter:
    """
    Project Gutenberg ë°ì´í„°ì…‹ ì„í¬í„°

    ChromaDB/Pineconeì— ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì„í¬íŠ¸í•˜ê³ 
    Spring Boot APIë¥¼ í†µí•´ PostgreSQL ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """

    # ì»¬ë ‰ì…˜ ì´ë¦„
    COLLECTIONS = [
        "novel_passages",
        "characters",
        "locations",
        "events",
        "themes"
    ]

    BATCH_SIZE = 1000
    MAX_RETRIES = 3

    def __init__(
        self,
        dataset_path: str,
        vectordb_type: str = "chromadb",
        vectordb_host: str = "localhost:8001",
        spring_boot_api: str = "http://localhost:8080",
        dry_run: bool = False
    ):
        """
        Args:
            dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ
            vectordb_type: VectorDB íƒ€ì… (chromadb, pinecone)
            vectordb_host: VectorDB í˜¸ìŠ¤íŠ¸
            spring_boot_api: Spring Boot API URL
            dry_run: Trueë©´ ì‹¤ì œ ì €ì¥ ì—†ì´ ê²€ì¦ë§Œ ìˆ˜í–‰
        """
        self.dataset_path = Path(dataset_path)
        self.vectordb_type = vectordb_type
        self.vectordb_host = vectordb_host
        self.spring_boot_api = spring_boot_api.rstrip("/")
        self.dry_run = dry_run

        self.stats = ImportStats()
        self._init_vectordb_client()

    def _init_vectordb_client(self):
        """VectorDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self.vectordb_type == "chromadb":
            # Docker í™˜ê²½ì—ì„œëŠ” HTTP í´ë¼ì´ì–¸íŠ¸, ë¡œì»¬ì—ì„œëŠ” Persistent í´ë¼ì´ì–¸íŠ¸
            if ":" in self.vectordb_host:
                host, port = self.vectordb_host.split(":")
                self.chroma_client = chromadb.HttpClient(
                    host=host,
                    port=int(port)
                )
            else:
                # ë¡œì»¬ persistent ëª¨ë“œ
                self.chroma_client = chromadb.PersistentClient(
                    path=self.vectordb_host
                )
        elif self.vectordb_type == "pinecone":
            # Pineconeì€ ë³„ë„ ì´ˆê¸°í™” í•„ìš”
            raise NotImplementedError("Pinecone ì§€ì›ì€ ì¶”í›„ êµ¬í˜„ ì˜ˆì •")
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” vectordb_type: {self.vectordb_type}")

    def import_all(self) -> ImportStats:
        """
        ì „ì²´ ë°ì´í„°ì…‹ ì„í¬íŠ¸ ìˆ˜í–‰

        Returns:
            ì„í¬íŠ¸ í†µê³„
        """
        start_time = time.time()

        print("ğŸš€ ë°ì´í„°ì…‹ ì„í¬íŠ¸ ì‹œì‘...")
        print(f"   ë°ì´í„°ì…‹ ê²½ë¡œ: {self.dataset_path}")
        print(f"   VectorDB: {self.vectordb_type} ({self.vectordb_host})")
        print(f"   Spring Boot API: {self.spring_boot_api}")
        print(f"   Dry Run: {self.dry_run}")
        print()

        # 1. ë°ì´í„°ì…‹ ê²€ì¦
        print("ğŸ“‹ Step 1/6: ë°ì´í„°ì…‹ ê²€ì¦ ì¤‘...")
        validator = DatasetValidator(str(self.dataset_path))
        validation_result = validator.validate()

        if not validation_result["valid"]:
            print("âŒ ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨:")
            for error in validation_result["errors"]:
                print(f"   - {error}")
            self.stats.errors.extend(validation_result["errors"])
            return self.stats

        print(f"âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ: {validation_result['stats']}")

        if validation_result["warnings"]:
            for warning in validation_result["warnings"]:
                print(f"âš ï¸  {warning}")
        print()

        # 2. ì»¬ë ‰ì…˜ ìƒì„±
        print("ğŸ“‹ Step 2/6: ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì¤‘...")
        self._create_collections()
        print()

        # 3. ì†Œì„¤ ë¡œë“œ
        print("ğŸ“‹ Step 3/6: novels.json ë¡œë“œ ì¤‘...")
        novels = self._load_novels()
        print(f"âœ… {len(novels)}ê°œ ì†Œì„¤ ë¡œë“œ ì™„ë£Œ")
        print()

        # 4. ê° ì†Œì„¤ë³„ ë°ì´í„° ì„í¬íŠ¸
        print("ğŸ“‹ Step 4/6: ì†Œì„¤ ë°ì´í„° ì„í¬íŠ¸ ì¤‘...")
        for novel in tqdm(novels, desc="ì†Œì„¤ ì„í¬íŠ¸", total=len(novels)):
            try:
                self._import_novel(novel)
                self.stats.novels_count += 1
            except Exception as e:
                error_msg = f"ì†Œì„¤ '{novel.get('title', 'Unknown')}' ì„í¬íŠ¸ ì‹¤íŒ¨: {e}"
                print(f"\nâŒ {error_msg}")
                self.stats.errors.append(error_msg)
        print()

        # 5. PostgreSQL ë©”íƒ€ë°ì´í„° ìƒì„±
        print("ğŸ“‹ Step 5/6: PostgreSQL ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...")
        for novel in novels:
            try:
                self._create_novel_metadata(novel)
            except Exception as e:
                error_msg = f"ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨ ('{novel.get('title')}'): {e}"
                print(f"\nâš ï¸  {error_msg}")
                self.stats.errors.append(error_msg)
        print()

        # 6. ê²€ì¦
        print("ğŸ“‹ Step 6/6: ì„í¬íŠ¸ ê²€ì¦ ì¤‘...")
        self._verify_import()

        elapsed_time = time.time() - start_time

        print()
        print("=" * 60)
        print("âœ… ë°ì´í„°ì…‹ ì„í¬íŠ¸ ì™„ë£Œ!")
        print("=" * 60)
        print(f"   ì†Œì„¤ ìˆ˜: {self.stats.novels_count}")
        print(f"   íŒ¨ì‹œì§€ ìˆ˜: {self.stats.passages_count}")
        print(f"   ìºë¦­í„° ìˆ˜: {self.stats.characters_count}")
        print(f"   ì¥ì†Œ ìˆ˜: {self.stats.locations_count}")
        print(f"   ì´ë²¤íŠ¸ ìˆ˜: {self.stats.events_count}")
        print(f"   í…Œë§ˆ ìˆ˜: {self.stats.themes_count}")
        print(f"   ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"   ì˜¤ë¥˜ ìˆ˜: {len(self.stats.errors)}")

        if self.stats.errors:
            print("\nâŒ ì˜¤ë¥˜ ëª©ë¡:")
            for error in self.stats.errors[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                print(f"   - {error}")
            if len(self.stats.errors) > 10:
                print(f"   ... ì™¸ {len(self.stats.errors) - 10}ê°œ")

        return self.stats

    def _create_collections(self):
        """ChromaDB ì»¬ë ‰ì…˜ ìƒì„±"""
        for collection_name in self.COLLECTIONS:
            try:
                if self.dry_run:
                    print(f"   [DRY RUN] ì»¬ë ‰ì…˜ ìƒì„± ê±´ë„ˆëœ€: {collection_name}")
                    continue

                self.chroma_client.get_or_create_collection(
                    name=collection_name,
                    metadata={"hnsw:space": "cosine"}  # HNSW ì¸ë±ìŠ¤, ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                )
                print(f"   âœ… ì»¬ë ‰ì…˜ ìƒì„±/í™•ì¸: {collection_name}")
            except Exception as e:
                print(f"   âš ï¸  ì»¬ë ‰ì…˜ {collection_name} ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    def _load_novels(self) -> List[Dict]:
        """novels.json ë¡œë“œ"""
        novels_path = self.dataset_path / "novels.json"
        with open(novels_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _import_novel(self, novel: Dict):
        """ë‹¨ì¼ ì†Œì„¤ ë°ì´í„° ì„í¬íŠ¸"""
        novel_id = novel["id"]

        # íŒ¨ì‹œì§€ ì„í¬íŠ¸
        self._import_passages(novel_id)

        # ìºë¦­í„° ì„í¬íŠ¸
        self._import_characters(novel_id)

        # ì¥ì†Œ ì„í¬íŠ¸ (ì„ íƒ)
        self._import_locations(novel_id)

        # ì´ë²¤íŠ¸ ì„í¬íŠ¸ (ì„ íƒ)
        self._import_events(novel_id)

        # í…Œë§ˆ ì„í¬íŠ¸ (ì„ íƒ)
        self._import_themes(novel_id)

    def _import_passages(self, novel_id: str):
        """íŒ¨ì‹œì§€ ë°ì´í„° ì„í¬íŠ¸"""
        # JSON ë˜ëŠ” Parquet íŒŒì¼ ì°¾ê¸°
        passages_dir = self.dataset_path / "passages"

        # íŒŒì¼ í˜•ì‹ ê²°ì •
        json_path = passages_dir / f"{novel_id}.json"
        parquet_path = passages_dir / f"{novel_id}.parquet"

        passages = []

        if parquet_path.exists():
            df = pd.read_parquet(parquet_path)
            passages = df.to_dict("records")
        elif json_path.exists():
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    passages = data
                elif isinstance(data, dict) and "passages" in data:
                    passages = data["passages"]
                elif isinstance(data, dict) and "chunks" in data:
                    # ê¸°ì¡´ í¬ë§· ì§€ì›
                    passages = data["chunks"]
        else:
            return  # íŒŒì¼ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€

        if not passages:
            return

        if self.dry_run:
            self.stats.passages_count += len(passages)
            return

        collection = self.chroma_client.get_collection("novel_passages")

        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(passages), self.BATCH_SIZE):
            batch = passages[i:i + self.BATCH_SIZE]

            ids = []
            embeddings = []
            documents = []
            metadatas = []

            for p in batch:
                # ID ìƒì„±
                passage_id = p.get("id") or f"{novel_id}_passage_{p.get('chunk_index', uuid.uuid4().hex[:8])}"
                ids.append(passage_id)

                # ì„ë² ë”©
                embedding = p.get("embedding")
                if embedding:
                    embeddings.append(embedding)

                # ë¬¸ì„œ í…ìŠ¤íŠ¸
                documents.append(p.get("text", ""))

                # ë©”íƒ€ë°ì´í„°
                metadatas.append({
                    "novel_id": novel_id,
                    "chapter_number": p.get("chapter_number", 0),
                    "passage_number": p.get("passage_number", p.get("chunk_index", 0)),
                    "word_count": p.get("word_count", 0),
                    "passage_type": p.get("passage_type", "narrative")
                })

            # ChromaDBì— ì¶”ê°€
            if embeddings:
                collection.add(
                    ids=ids,
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas
                )

            self.stats.passages_count += len(batch)

    def _import_characters(self, novel_id: str):
        """ìºë¦­í„° ë°ì´í„° ì„í¬íŠ¸"""
        chars_path = self.dataset_path / "characters" / f"{novel_id}.json"

        if not chars_path.exists():
            return

        with open(chars_path, "r", encoding="utf-8") as f:
            characters = json.load(f)

        if not characters:
            return

        if self.dry_run:
            self.stats.characters_count += len(characters)
            return

        collection = self.chroma_client.get_collection("characters")

        ids = []
        embeddings = []
        documents = []
        metadatas = []

        for char in characters:
            char_id = char.get("id") or f"{novel_id}_char_{char.get('name', 'unknown')}"
            ids.append(char_id)

            # ì„ë² ë”©
            if char.get("embedding"):
                embeddings.append(char["embedding"])

            # ë¬¸ì„œ (ìºë¦­í„° ì„¤ëª…)
            documents.append(char.get("description", ""))

            # personality_traitsë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            personality_traits = char.get("personality_traits", [])
            if isinstance(personality_traits, list):
                personality_traits_str = json.dumps(personality_traits)
            else:
                personality_traits_str = str(personality_traits)

            metadatas.append({
                "novel_id": novel_id,
                "name": char.get("name", ""),
                "role": char.get("role", "supporting"),
                "personality_traits": personality_traits_str,
                "first_appearance_chapter": char.get("first_appearance_chapter", 1)
            })

        if embeddings:
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

        self.stats.characters_count += len(characters)

    def _import_locations(self, novel_id: str):
        """ì¥ì†Œ ë°ì´í„° ì„í¬íŠ¸"""
        locations_path = self.dataset_path / "locations" / f"{novel_id}.json"

        if not locations_path.exists():
            return

        with open(locations_path, "r", encoding="utf-8") as f:
            locations = json.load(f)

        if not locations or self.dry_run:
            if locations:
                self.stats.locations_count += len(locations)
            return

        collection = self.chroma_client.get_collection("locations")

        ids = [loc.get("id") or f"{novel_id}_loc_{i}" for i, loc in enumerate(locations)]
        embeddings = [loc["embedding"] for loc in locations if loc.get("embedding")]
        documents = [loc.get("description", "") for loc in locations]
        metadatas = [{"novel_id": novel_id, "name": loc.get("name", "")} for loc in locations]

        if embeddings:
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

        self.stats.locations_count += len(locations)

    def _import_events(self, novel_id: str):
        """ì´ë²¤íŠ¸ ë°ì´í„° ì„í¬íŠ¸"""
        events_path = self.dataset_path / "events" / f"{novel_id}.json"

        if not events_path.exists():
            return

        with open(events_path, "r", encoding="utf-8") as f:
            events = json.load(f)

        if not events or self.dry_run:
            if events:
                self.stats.events_count += len(events)
            return

        collection = self.chroma_client.get_collection("events")

        ids = [evt.get("id") or f"{novel_id}_evt_{i}" for i, evt in enumerate(events)]
        embeddings = [evt["embedding"] for evt in events if evt.get("embedding")]
        documents = [evt.get("description", "") for evt in events]
        metadatas = [{"novel_id": novel_id, "name": evt.get("name", "")} for evt in events]

        if embeddings:
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

        self.stats.events_count += len(events)

    def _import_themes(self, novel_id: str):
        """í…Œë§ˆ ë°ì´í„° ì„í¬íŠ¸"""
        themes_path = self.dataset_path / "themes" / f"{novel_id}.json"

        if not themes_path.exists():
            return

        with open(themes_path, "r", encoding="utf-8") as f:
            themes = json.load(f)

        if not themes or self.dry_run:
            if themes:
                self.stats.themes_count += len(themes)
            return

        collection = self.chroma_client.get_collection("themes")

        ids = [thm.get("id") or f"{novel_id}_thm_{i}" for i, thm in enumerate(themes)]
        embeddings = [thm["embedding"] for thm in themes if thm.get("embedding")]
        documents = [thm.get("description", "") for thm in themes]
        metadatas = [{"novel_id": novel_id, "name": thm.get("name", "")} for thm in themes]

        if embeddings:
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

        self.stats.themes_count += len(themes)

    def _create_novel_metadata(self, novel: Dict):
        """Spring Boot APIë¥¼ í†µí•´ PostgreSQL ë©”íƒ€ë°ì´í„° ìƒì„±"""
        if self.dry_run:
            print(f"   [DRY RUN] ë©”íƒ€ë°ì´í„° ìƒì„± ê±´ë„ˆëœ€: {novel.get('title')}")
            return

        # íŒ¨ì‹œì§€ì™€ ìºë¦­í„° ìˆ˜ ê³„ì‚°
        passages_count = 0
        characters_count = 0

        passages_dir = self.dataset_path / "passages"
        if (passages_dir / f"{novel['id']}.parquet").exists():
            df = pd.read_parquet(passages_dir / f"{novel['id']}.parquet")
            passages_count = len(df)
        elif (passages_dir / f"{novel['id']}.json").exists():
            with open(passages_dir / f"{novel['id']}.json", "r") as f:
                data = json.load(f)
                if isinstance(data, list):
                    passages_count = len(data)
                elif isinstance(data, dict):
                    passages_count = len(data.get("passages", data.get("chunks", [])))

        chars_path = self.dataset_path / "characters" / f"{novel['id']}.json"
        if chars_path.exists():
            with open(chars_path, "r") as f:
                chars = json.load(f)
                characters_count = len(chars)

        payload = {
            "title": novel.get("title", "Unknown"),
            "author": novel.get("author", "Unknown"),
            "publication_year": novel.get("publication_year"),
            "genre": novel.get("genre", ""),
            "language": novel.get("language", "en"),
            "vectordb_collection_id": novel["id"],
            "total_passages_count": passages_count,
            "total_characters_count": characters_count,
            "ingestion_status": "completed"
        }

        for attempt in range(self.MAX_RETRIES):
            try:
                with httpx.Client(timeout=30.0) as client:
                    response = client.post(
                        f"{self.spring_boot_api}/api/internal/novels",
                        json=payload,
                        headers={"Content-Type": "application/json"}
                    )

                    if response.status_code in (200, 201):
                        result = response.json()
                        print(f"   âœ… ë©”íƒ€ë°ì´í„° ìƒì„±: {novel.get('title')} (ID: {result.get('id', 'N/A')})")
                        return result
                    elif response.status_code == 409:
                        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                        print(f"   âš ï¸  ì´ë¯¸ ì¡´ì¬: {novel.get('title')}")
                        return None
                    else:
                        print(f"   âš ï¸  API ì‘ë‹µ ì˜¤ë¥˜ ({response.status_code}): {response.text[:100]}")

            except httpx.ConnectError:
                if attempt < self.MAX_RETRIES - 1:
                    print(f"   âš ï¸  Spring Boot ì—°ê²° ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{self.MAX_RETRIES})")
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    print(f"   âŒ Spring Boot API ì—°ê²° ì‹¤íŒ¨: {self.spring_boot_api}")
                    self.stats.errors.append(f"Spring Boot ì—°ê²° ì‹¤íŒ¨: {novel.get('title')}")

            except Exception as e:
                self.stats.errors.append(f"ë©”íƒ€ë°ì´í„° ìƒì„± ì˜¤ë¥˜ ({novel.get('title')}): {e}")
                break

        return None

    def _verify_import(self):
        """ì„í¬íŠ¸ ê²€ì¦"""
        print("\nğŸ” ì„í¬íŠ¸ ê²€ì¦ ê²°ê³¼:")

        for collection_name in self.COLLECTIONS:
            try:
                collection = self.chroma_client.get_collection(collection_name)
                count = collection.count()
                print(f"   âœ… {collection_name}: {count}ê°œ ë¬¸ì„œ")
            except Exception as e:
                print(f"   âŒ {collection_name}: ì˜¤ë¥˜ - {e}")


def main():
    parser = argparse.ArgumentParser(
        description="Project Gutenberg ë°ì´í„°ì…‹ì„ VectorDBì— ì„í¬íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
    # ChromaDB (ë¡œì»¬)
    python scripts/import_dataset.py \\
        --dataset-path ./data/gutenberg_dataset \\
        --vectordb chromadb \\
        --vectordb-host ./chroma_data

    # ChromaDB (Docker)
    python scripts/import_dataset.py \\
        --dataset-path ./data/gutenberg_dataset \\
        --vectordb chromadb \\
        --vectordb-host localhost:8001 \\
        --spring-boot-api http://localhost:8080

    # Dry Run (ê²€ì¦ë§Œ)
    python scripts/import_dataset.py \\
        --dataset-path ./data/gutenberg_dataset \\
        --dry-run
        """
    )

    parser.add_argument(
        "--dataset-path",
        required=True,
        help="ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--vectordb",
        choices=["chromadb", "pinecone"],
        default="chromadb",
        help="VectorDB íƒ€ì… (ê¸°ë³¸: chromadb)"
    )
    parser.add_argument(
        "--vectordb-host",
        default="localhost:8001",
        help="VectorDB í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸: localhost:8001)"
    )
    parser.add_argument(
        "--spring-boot-api",
        default="http://localhost:8080",
        help="Spring Boot API URL (ê¸°ë³¸: http://localhost:8080)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ê²€ì¦ë§Œ ìˆ˜í–‰í•˜ê³  ì‹¤ì œ ì„í¬íŠ¸ëŠ” ê±´ë„ˆëœ€"
    )
    parser.add_argument(
        "--validate-only",
        action="store_true",
        help="ë°ì´í„°ì…‹ ê²€ì¦ë§Œ ìˆ˜í–‰"
    )

    args = parser.parse_args()

    # ê²€ì¦ë§Œ ìˆ˜í–‰
    if args.validate_only:
        print("ğŸ“‹ ë°ì´í„°ì…‹ ê²€ì¦ ì¤‘...")
        validator = DatasetValidator(args.dataset_path)
        result = validator.validate()

        if result["valid"]:
            print("âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì„±ê³µ!")
            print(f"   í†µê³„: {result['stats']}")
        else:
            print("âŒ ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨!")
            for error in result["errors"]:
                print(f"   - {error}")

        if result["warnings"]:
            print("\nâš ï¸  ê²½ê³ :")
            for warning in result["warnings"]:
                print(f"   - {warning}")

        sys.exit(0 if result["valid"] else 1)

    # ì „ì²´ ì„í¬íŠ¸ ìˆ˜í–‰
    importer = GutenbergDatasetImporter(
        dataset_path=args.dataset_path,
        vectordb_type=args.vectordb,
        vectordb_host=args.vectordb_host,
        spring_boot_api=args.spring_boot_api,
        dry_run=args.dry_run
    )

    stats = importer.import_all()

    # ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ exit code 1
    sys.exit(1 if stats.errors else 0)


if __name__ == "__main__":
    main()
